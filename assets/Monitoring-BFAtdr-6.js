import{j as e,u as n,a as i,D as t,V as o,d as s,I as a,b as r}from"./three-V9KlXHdT.js";import{B as c,P as l,F as u}from"./Footer-jmf9Sd_V.js";import{m,F as d}from"./FeaturesContainer-KvHSRds3.js";import{b as f}from"./vendor-CPR9a7GK.js";import{v as p,V as h}from"./VideoIcon-C4Uaqrgn.js";import{W as v,b as g}from"./main-Cb_urJEy.js";import"./gsap-CykRABAj.js";function y(){const[n,i]=f.useState(!1);return e.jsxs("div",{className:"hero-section section",children:[e.jsxs("div",{className:"hero-content",children:[e.jsx("img",{src:m,alt:"Monitoring Logo",className:"logo"}),e.jsx("div",{className:"hero-title",children:"Monitoring"}),e.jsx("p",{className:"hero-description",children:"Our system aligns every aspect of monitoring with your specific goals and needs. Define what matters to you, and our AI focuses exclusively on those objectives, delivering precisely what you need to know."}),e.jsxs("div",{className:"hero-buttons",children:[e.jsx(c,{label:"Book a Demo",isPrimary:!0}),e.jsx(c,{label:"watch a promo",isPrimary:!0,Outline:!0,imgSrc:p,onClick:()=>{i(!0)}})]})]}),e.jsx(h,{isVisible:n,onClose:()=>{i(!1)},vimeoUrl:"https://player.vimeo.com/video/1034502434"})]})}const x=.025,b=8.5,w=10.5,P=!1,S=150,C=f.forwardRef(function({geoComplexity:r=136,meshType:c=new a(100,r),meshPositionX:u=0,meshPositionY:m=0,meshPositionZ:d=0,meshRotationX:p=0,meshRotationY:h=0,meshRotationZ:v=0,frequency:g=x,setFrequency:y,amplitude:C=b,setAmplitude:j,maxDistance:k=w,setMaxDistance:D,timeSpeed:z=.5,noiseScale:R=1,noiseDensity:N=1,noiseOctaves:O=3,noiseLacunarity:F=2,noiseGain:I=.5,turbulenceStrength:q=1,flowDirection:_=0,waveSpeed:T=1,distortionStrength:A=1,particleSize:M=2,colorIntensity:Z=1,particleColor:X=[1,1,1],nearFadeDistance:Y=50,farFadeDistance:B=180,mouseInfluenceStrength:G=1,mouseOrderRadius:V=.3,mouseOrderStrength:E=.8,chaosStrength:L=2,isMobile:W=P,setIsMobile:K,cameraZ:U=S,setCameraZ:H},Q){const J=f.useRef(),$=f.useRef({x:.5,y:.5}),ee=f.useRef(0),[ne,ie]=f.useState(G),[te,oe]=f.useState(V),[se,ae]=f.useState(E),[re,ce]=f.useState(L);return f.useEffect(()=>{ie(G),oe(V),ae(E),ce(L)},[G,V,E,L]),n(({camera:e})=>{e.position.z=U}),i(({clock:e,size:n})=>{ee.current=e.elapsedTime,J.current&&(J.current.uniforms.u_time.value=e.elapsedTime*z,J.current.uniforms.u_resolution.value.set(n.width,n.height),J.current.uniforms.u_mouse.value.set($.current.x,$.current.y),J.current.uniforms.noiseScale.value=R,J.current.uniforms.noiseDensity.value=N,J.current.uniforms.noiseOctaves.value=O,J.current.uniforms.noiseLacunarity.value=F,J.current.uniforms.noiseGain.value=I,J.current.uniforms.turbulenceStrength.value=q,J.current.uniforms.flowDirection.value=_,J.current.uniforms.waveSpeed.value=T,J.current.uniforms.distortionStrength.value=A,J.current.uniforms.u_mouseInfluence.value=ne,J.current.uniforms.mouseOrderRadius.value=te,J.current.uniforms.mouseOrderStrength.value=se,J.current.uniforms.chaosStrength.value=re,J.current.uniforms.particleSize.value=M,J.current.uniforms.colorIntensity.value=Z,J.current.uniforms.particleColor.value.set(1,1,1),J.current.uniforms.nearFadeDistance.value=Y,J.current.uniforms.farFadeDistance.value=B),void 0!==g&&y&&(y(g),J.current&&(J.current.uniforms.frequency.value=g)),void 0!==C&&j&&(j(C),J.current&&(J.current.uniforms.amplitude.value=C)),void 0!==k&&D&&(D(k),J.current&&(J.current.uniforms.maxDistance.value=k)),void 0!==W&&K&&(K(W),J.current&&(J.current.uniforms.isMobile.value=W)),void 0!==U&&H&&H(U)}),f.useEffect(()=>{function e(e){const n=document.querySelector("#monitoring-sketch canvas");if(!n)return;const i=n.getBoundingClientRect();$.current.x=(e.clientX-i.left)/i.width,$.current.y=1-(e.clientY-i.top)/i.height}return document.addEventListener("mousemove",e),()=>document.removeEventListener("mousemove",e)},[]),f.useEffect(()=>{J.current&&(J.current.uniforms.noiseScale.value=R,J.current.uniforms.noiseDensity.value=N,J.current.uniforms.noiseOctaves.value=O,J.current.uniforms.noiseLacunarity.value=F,J.current.uniforms.noiseGain.value=I,J.current.uniforms.turbulenceStrength.value=q,J.current.uniforms.flowDirection.value=_,J.current.uniforms.waveSpeed.value=T,J.current.uniforms.distortionStrength.value=A)},[R,N,O,F,I,q,_,T,A]),f.useEffect(()=>{J.current&&(J.current.uniforms.mouseOrderRadius.value=V,J.current.uniforms.mouseOrderStrength.value=E,J.current.uniforms.chaosStrength.value=L)},[V,E,L]),f.useEffect(()=>{J.current&&(J.current.uniforms.particleSize.value=M,J.current.uniforms.colorIntensity.value=Z,J.current.uniforms.particleColor.value.set(1,1,1),J.current.uniforms.nearFadeDistance.value=Y,J.current.uniforms.farFadeDistance.value=B)},[M,Z,X,Y,B]),f.useEffect(()=>{},[]),e.jsxs("points",{ref:Q,position:[u,m,d],rotation:[p,h,v],children:[e.jsx(f.Suspense,{fallback:e.jsx("icosahedronGeometry",{args:[100,20]}),children:e.jsx(l,{})}),e.jsx("shaderMaterial",{ref:J,uniforms:{u_time:{value:0},u_resolution:{value:new o},frequency:{type:"f",value:g},amplitude:{type:"f",value:C},maxDistance:{type:"f",value:k},noiseScale:{type:"f",value:R},noiseDensity:{type:"f",value:N},noiseOctaves:{type:"f",value:O},noiseLacunarity:{type:"f",value:F},noiseGain:{type:"f",value:I},turbulenceStrength:{type:"f",value:q},flowDirection:{type:"f",value:_},waveSpeed:{type:"f",value:T},distortionStrength:{type:"f",value:A},particleSize:{type:"f",value:M},colorIntensity:{type:"f",value:Z},particleColor:{value:new s(1,1,1)},nearFadeDistance:{type:"f",value:Y},farFadeDistance:{type:"f",value:B},u_mouse:{value:new o(.5,.5)},u_mouseInfluence:{value:ne},mouseOrderRadius:{type:"f",value:te},mouseOrderStrength:{type:"f",value:se},chaosStrength:{type:"f",value:re},isMobile:{type:"bool",value:W},uPositions:{value:null}},side:t,vertexShader:"uniform float u_time;\nuniform vec2 u_resolution;\nuniform vec2 u_mouse;\n\nuniform float mouseOrderRadius;\nuniform float mouseOrderStrength;\nuniform float chaosStrength;\n\nuniform float frequency;\nuniform float amplitude;\nuniform float maxDistance;\n\nuniform float noiseScale;\nuniform float noiseDensity;\nuniform float noiseOctaves;\nuniform float noiseLacunarity;\nuniform float noiseGain;\nuniform float turbulenceStrength;\nuniform float flowDirection;\nuniform float waveSpeed;\nuniform float distortionStrength;\n\nuniform float particleSize;\nuniform float colorIntensity;\n\nuniform float nearFadeDistance;\nuniform float farFadeDistance;\n\nuniform bool isMobile;\nuniform sampler2D uPositions;\n\nvarying vec2 vUv;\nvarying vec3 vPosition;\nattribute vec2 reference;\n\nout vec3 vertexPosition;\nout vec2 textureCoord;\nout float distToCamera;\n\nfloat PI = 3.141592653589793238;\n\nfloat rand(vec2 c){\n    return fract(sin(dot(c.xy ,vec2(12.9898,78.233))) * 43758.5453);\n}\n\nfloat noise(vec2 p, float freq ){\n    float unit = 1./freq;\n    vec2 ij = floor(p/unit);\n    vec2 xy = mod(p,unit)/unit;\n    xy = .5*(1.-cos(PI*xy));\n    float a = rand((ij+vec2(0.,0.)));\n    float b = rand((ij+vec2(1.,0.)));\n    float c = rand((ij+vec2(0.,1.)));\n    float d = rand((ij+vec2(1.,1.)));\n    float x1 = mix(a, b, xy.x);\n    float x2 = mix(c, d, xy.x);\n    return mix(x1, x2, xy.y);\n}\n\nvec3 mod289(vec3 x) {\n    return x - floor(x * (1.0 / 289.0)) * 289.0;\n}\n\nvec2 mod289(vec2 x) {\n    return x - floor(x * (1.0 / 289.0)) * 289.0;\n}\n\nvec3 permute(vec3 x) {\n    return mod289(((x*34.0)+1.0)*x);\n}\n\nfloat noise(vec2 v) {\n    const vec4 C = vec4(0.211324865405187,  \n                        0.366025403784439,  \n                    -0.577350269189626,  \n                        0.024390243902439); \n    \n    vec2 i  = floor(v + dot(v, C.yy) );\n    vec2 x0 = v -   i + dot(i, C.xx);\n\n    \n    vec2 i1;\n    i1 = (x0.x > x0.y) ? vec2(1.0, 0.0) : vec2(0.0, 1.0);\n    vec4 x12 = x0.xyxy + C.xxzz;\n    x12.xy -= i1;\n\n    \n    i = mod289(i); \n    vec3 p = permute( permute( i.y + vec3(0.0, i1.y, 1.0 ))\n        + i.x + vec3(0.0, i1.x, 1.0 ));\n\n    vec3 m = max(0.5 - vec3(dot(x0,x0), dot(x12.xy,x12.xy), dot(x12.zw,x12.zw)), 0.0);\n    m = m*m ;\n    m = m*m ;\n\n    vec3 x = 2.0 * fract(p * C.www) - 1.0;\n    vec3 h = abs(x) - 0.5;\n    vec3 ox = floor(x + 0.5);\n    vec3 a0 = x - ox;\n\n    m *= 1.79284291400159 - 0.85373472095314 * ( a0*a0 + h*h );\n\n    vec3 g;\n    g.x  = a0.x  * x0.x  + h.x  * x0.y;\n    g.yz = a0.yz * x12.xz + h.yz * x12.yw;\n    return 130.0 * dot(m, g);\n}\n\nvec3 curl(float x, float y, float z) {\n    float eps = 1.0 / noiseDensity, eps2 = 2.0 * eps;\n    float n1, n2, a, b;\n\n    \n    float timeOffset = u_time * waveSpeed * 0.05;\n    x += timeOffset;\n    y += timeOffset;\n    z += timeOffset;\n\n    \n    float flowRad = flowDirection * PI / 180.0;\n    vec2 flowVec = vec2(cos(flowRad), sin(flowRad)) * 0.1;\n    x += flowVec.x * u_time * waveSpeed;\n    y += flowVec.y * u_time * waveSpeed;\n\n    \n    x *= noiseScale;\n    y *= noiseScale;\n    z *= noiseScale;\n\n    vec3 curl = vec3(0.0);\n\n    \n    n1 = noise(vec2(x, y + eps));\n    n2 = noise(vec2(x, y - eps));\n    a = (n1 - n2) / eps2;\n\n    n1 = noise(vec2(x, z + eps));\n    n2 = noise(vec2(x, z - eps));\n    b = (n1 - n2) / eps2;\n\n    curl.x = (a - b) * turbulenceStrength;\n\n    n1 = noise(vec2(y, z + eps));\n    n2 = noise(vec2(y, z - eps));\n    a = (n1 - n2) / eps2;\n\n    n1 = noise(vec2(x + eps, z));\n    n2 = noise(vec2(x - eps, z));\n    b = (n1 - n2) / eps2;\n\n    curl.y = (a - b) * turbulenceStrength;\n\n    n1 = noise(vec2(x + eps, y));\n    n2 = noise(vec2(x - eps, y));\n    a = (n1 - n2) / eps2;\n\n    n1 = noise(vec2(y + eps, z));\n    n2 = noise(vec2(y - eps, z));\n    b = (n1 - n2) / eps2;\n\n    curl.z = (a - b) * turbulenceStrength;\n\n    return curl;\n}\n\nfloat fbm(vec2 p) {\n    float value = 0.0;\n    float amplitude_local = noiseGain;\n    float frequency_local = 1.0;\n    \n    for (int i = 0; i < int(noiseOctaves); i++) {\n        value += amplitude_local * noise(p * frequency_local);\n        p *= noiseLacunarity;\n        amplitude_local *= noiseGain;\n    }\n    return value;\n}\n\nfloat calculateMouseInfluence(vec3 position) {\n    \n    vec4 mvPosition = modelViewMatrix * vec4(position, 1.0);\n    vec4 projPosition = projectionMatrix * mvPosition;\n    \n    \n    vec2 ndc = projPosition.xy / projPosition.w;\n    \n    \n    vec2 screenPos = ndc * 0.5 + 0.5;\n    \n    \n    vec2 mousePos = u_mouse;\n    \n    \n    float dist = distance(screenPos, mousePos);\n    \n    \n    float influence = 1.0 - smoothstep(0.0, mouseOrderRadius, dist);\n    \n    return influence;\n}\n\nvoid main() {\n    vUv = reference;\n    vPosition = position;\n\n    vec4 cs_position = modelViewMatrix * vec4(position, 1.0);\n    distToCamera = -cs_position.z;\n\n    vec3 newPos = position;\n    \n    \n    float mouseInfluence = calculateMouseInfluence(position);\n    \n    \n    vec3 chaosNoisePos = newPos * frequency * noiseScale;\n    vec3 chaosCurlForce = curl(chaosNoisePos.x, chaosNoisePos.y, chaosNoisePos.z);\n    \n    \n    float chaosFbmNoise = fbm(chaosNoisePos.xy + u_time * waveSpeed * 0.1);\n    chaosCurlForce += vec3(chaosFbmNoise) * 0.8;\n    \n    \n    vec3 chaosTarget = position + chaosCurlForce * amplitude * chaosStrength * distortionStrength;\n    \n    \n    vec3 orderNoisePos = newPos * frequency * 0.1; \n    vec3 orderCurlForce = curl(orderNoisePos.x, orderNoisePos.y, orderNoisePos.z);\n    vec3 orderTarget = position + orderCurlForce * amplitude * 0.1; \n    \n    \n    vec3 blendedTarget = mix(chaosTarget, orderTarget, mouseInfluence * mouseOrderStrength);\n    \n    \n    float d = length(position - blendedTarget) / maxDistance;\n    vec3 finalPos = mix(position, blendedTarget, pow(d, 5.0));\n\n    vec4 mvPosition = modelViewMatrix * vec4(finalPos, 1.0);\n    \n    \n    \n    float distanceScale = 1.0 - smoothstep(nearFadeDistance, farFadeDistance, -mvPosition.z);\n    distanceScale = 0.3 + distanceScale * 0.7; \n    \n    \n    gl_PointSize = particleSize * distanceScale * (1.0 / -mvPosition.z);\n    if (isMobile) {\n        gl_PointSize = particleSize * 2.0 * distanceScale * (1.0 / -mvPosition.z);\n    }\n    \n    gl_Position = projectionMatrix * mvPosition;\n}",fragmentShader:"uniform float u_time;\nuniform vec2 u_resolution;\nuniform vec2 u_mouse;\nuniform float colorIntensity;\nuniform vec3 particleColor;\n\nuniform float mouseOrderRadius;\nuniform float mouseOrderStrength;\nuniform float chaosStrength;\n\nuniform float nearFadeDistance;\nuniform float farFadeDistance;\n\nvarying vec2 vUv;\nvarying vec3 vPosition;\n\nin vec3 vertexPosition;\nin vec2 textureCoord;\nin float distToCamera;\n\nvoid main() {\n    vec2 st = gl_PointCoord.xy;\n    \n    \n    float dist = distance(st, vec2(0.5));\n    float alpha = 1.0 - smoothstep(0.0, 0.5, dist);\n    \n    if (alpha < 0.01) discard;\n    \n    \n    \n    vec2 screenPos = gl_FragCoord.xy / u_resolution.xy;\n    \n    \n    vec2 mousePos = u_mouse;\n    \n    \n    float mouseDistance = length(screenPos - mousePos);\n    \n    \n    float influenceRadius = mouseOrderRadius;\n    \n    \n    float mouseInfluence = 1.0 - smoothstep(0.0, influenceRadius, mouseDistance);\n    \n    \n    \n    \n    \n    \n    vec3 chaoticColor = vec3(1.0, 0.98, 0.95);\n    \n    \n    vec3 orderedColor = vec3(0.3, 0.7, 1.0);\n    \n    \n    vec3 baseColor = mix(chaoticColor, orderedColor, mouseInfluence * mouseOrderStrength);\n    \n    \n    \n    float orderBrightness = 1.0 + mouseInfluence * 0.4;\n    vec3 finalColor = baseColor * colorIntensity * orderBrightness;\n    \n    \n    float chaosFlicker = 1.0 - mouseInfluence * 0.7; \n    float flicker = 1.0 + sin(u_time * 8.0 + gl_PointCoord.x * 40.0) * 0.08 * chaosFlicker;\n    finalColor *= flicker;\n    \n    \n    \n    float distanceAlpha = 1.0 - smoothstep(nearFadeDistance, farFadeDistance, distToCamera);\n    \n    \n    distanceAlpha = pow(distanceAlpha, 1.5); \n    \n    \n    float finalAlpha = alpha * distanceAlpha;\n    \n    \n    finalAlpha = finalAlpha * 0.4 + smoothstep(0.0, 1.0, finalAlpha) * 0.5 + 0.1 * smoothstep(0.9 - fwidth(finalAlpha), 0.9, finalAlpha);\n    \n    gl_FragColor = vec4(finalColor, finalAlpha);\n}"})]})});function j({cameraPositionX:e=0,cameraPositionY:i=0,cameraPositionZ:t=200,meshPositionX:o=0,meshPositionY:s=0,meshPositionZ:a=0}){const{camera:r}=n();return f.useEffect(()=>{r.position.set(e,i,t),r.lookAt(o,s,a),r.updateProjectionMatrix()},[r,e,i,t,o,s,a]),null}function k({cameraPositionX:n,cameraPositionY:i,cameraPositionZ:t,meshPositionX:o,meshPositionY:s,meshPositionZ:a,meshRotationX:c,meshRotationY:l,meshRotationZ:u,...m}){return e.jsx(v,{children:e.jsx(r,{id:"monitoring-sketch",className:"sketch-container",style:{position:"fixed",top:0,left:0,width:"100vw",height:"100vh",zIndex:0,pointerEvents:"auto"},resize:{scroll:!1,debounce:{scroll:50,resize:50}},gl:{antialias:!0,alpha:!0,powerPreference:"high-performance",preserveDrawingBuffer:!1,failIfMajorPerformanceCaveat:!1},children:e.jsxs(f.Suspense,{fallback:null,children:[e.jsx(j,{cameraPositionX:n,cameraPositionY:i,cameraPositionZ:t,meshPositionX:o,meshPositionY:s,meshPositionZ:a}),e.jsx(C,{meshPositionX:o,meshPositionY:s,meshPositionZ:a,meshRotationX:c,meshRotationY:l,meshRotationZ:u,...m})]})})})}const D=[{title:"Objective Oriented Approach",description:"Got a goal? Our monitoring understands your objectives and delivers precisely the insights that matter to you, filtering out everything else to keep you focused on what's important.",image:"/ROKKA-Website-Dev/assets/Objective-B7UNdrb-.svg",imageClass:"image-portrait",byNeed:{political:{title:"Political Need",description:"Political operatives use objective-oriented monitoring to focus on campaign-specific goals such as voter sentiment analysis, opposition research, or issue tracking. Campaigns can tailor their monitoring to specific demographics, geographic regions, or policy topics that align with their electoral strategy, ensuring that social media intelligence directly supports campaign messaging and voter outreach efforts."},social:{title:"Intelligence Need",description:"Intelligence analysts employ objective-oriented monitoring to focus on specific threats, entities, or information requirements. By defining clear intelligence objectives, analysts can filter through vast amounts of social media data to identify relevant security threats, track specific individuals or organizations, or monitor emerging situations that could impact national security or organizational safety."},commercial:{title:"Commercial Need",description:"Businesses use objective-oriented monitoring to align social media intelligence with specific marketing goals, brand management objectives, or competitive analysis targets. Companies can focus on metrics that matter most to their bottom line - whether tracking brand sentiment, monitoring competitor activities, or measuring campaign effectiveness. This approach ensures that social media insights directly support business strategy and decision-making processes."},research:{title:"Research Need",description:"Researchers leverage objective-oriented monitoring to focus their data collection on specific research questions and hypotheses. This targeted approach ensures that social media analysis aligns with research objectives, whether studying public opinion trends, behavioral patterns, or communication dynamics. Researchers can filter out irrelevant noise and concentrate on data that directly addresses their scholarly inquiries, improving the validity and reliability of their findings."}}},{title:"Monitor Any Source",description:"Look beyond traditional data sources. Our technology scans and analyzes content across the entire web, finding valuable insights wherever they exist - from social media to specialized platforms.",image:"/ROKKA-Website-Dev/assets/AnySource-BSEn8QhR.svg",imageClass:"image-landscape",byNeed:{political:{title:"Political Need",description:"Political campaigns use comprehensive source monitoring to track public opinion across all digital channels where voters engage. This includes mainstream social media, local community forums, news comment sections, and emerging platforms popular with specific voter demographics. Comprehensive monitoring ensures campaigns understand the full spectrum of public discourse and can respond to emerging narratives regardless of where they originate."},social:{title:"Intelligence Need",description:"Intelligence professionals require comprehensive source monitoring to avoid blind spots in their information gathering. Threats and important intelligence can emerge from any platform, including encrypted messaging apps, dark web forums, specialized communities, or regional platforms. Multi-source monitoring ensures complete situational awareness and prevents critical intelligence from being missed due to platform limitations."},commercial:{title:"Commercial Need",description:"Businesses use multi-source monitoring to gain a complete view of their brand presence and customer conversations across the entire digital ecosystem. This includes monitoring review sites, forums, blogs, news outlets, and emerging platforms where customers might discuss products or services. Companies can identify new market opportunities, track brand mentions in unexpected contexts, and ensure comprehensive reputation management."},research:{title:"Research Need",description:"Academic researchers benefit from comprehensive source monitoring to ensure their studies capture diverse perspectives and avoid sampling bias. By monitoring beyond traditional platforms, researchers can access specialized communities, niche forums, and emerging platforms where specific demographics or interest groups congregate. This comprehensive approach enhances the generalizability and robustness of research findings across different digital environments."}}},{title:"Data Collection (Tracker)",description:"Follow specific influencers, thought leaders, or key figures that matter to your organization. Get real-time insights about what they're saying and how they're affecting your market.",image:"/ROKKA-Website-Dev/assets/Tracker-BwWVFhcI.svg",imageClass:"image-square",byNeed:{political:{title:"Political Need",description:"Political campaigns track key political figures, journalists, activists, and influential voices who shape public opinion and media narratives. This tracking helps campaigns anticipate opposition moves, identify media opportunities, build relationships with key influencers, and respond quickly to emerging political developments that could impact electoral outcomes."},social:{title:"Intelligence Need",description:"Intelligence analysts use tracking to monitor persons of interest, organizations, or developing situations that pose potential security concerns. Continuous tracking enables the identification of behavioral patterns, network connections, and emerging threats. This capability is crucial for threat assessment, counter-intelligence operations, and maintaining awareness of evolving security situations."},commercial:{title:"Commercial Need",description:"Companies employ tracking to monitor key influencers, brand ambassadors, competitors, and industry thought leaders who significantly impact their market space. By tracking these key figures, businesses can anticipate market trends, identify partnership opportunities, respond to competitive moves, and leverage influencer relationships for marketing purposes. This targeted tracking provides actionable intelligence for strategic business decisions."},research:{title:"Research Need",description:"Researchers use tracking capabilities to follow specific individuals, hashtags, or topics over extended periods, enabling longitudinal studies of social phenomena. This allows for the analysis of how opinions evolve, how information spreads through networks, and how key figures influence public discourse. Tracking provides the temporal data necessary for understanding causation and correlation in social media research."}}},{title:"Post Analysis",description:"Get instant analysis of every post as it appears, understanding its impact and relevance immediately. Plus, dive deeper with custom questions about any aspect of your data - from broad trends to specific posts. Generate strategic reports or get immediate answers about what matters to you.",image:"/ROKKA-Website-Dev/assets/PostAnalysis-wuasVNTM.svg",imageClass:"image-landscape",byNeed:{political:{title:"Political Need",description:'Campaigns create election-focused analytical questions that align with their specific electoral strategy and voter research needs. A campaign might ask "How do voters in key demographics respond to policy announcements?" or "What concerns are most frequently mentioned by undecided voters?" These custom queries direct analysis toward voter sentiment and political intelligence that directly supports campaign messaging, targeting decisions, and strategic positioning efforts.'},social:{title:"Intelligence Need",description:'Analysts formulate intelligence-specific questions that focus on their particular security concerns and information requirements. An analyst might ask "What coordination patterns appear in suspect communications?" or "How are threat narratives evolving over time?" These targeted queries direct the analytical process toward security-relevant insights, enabling efficient processing of large data volumes while maintaining focus on mission-critical intelligence objectives.'},commercial:{title:"Commercial Need",description:'Companies configure business-focused analytical questions that target their specific commercial intelligence needs. A tech company might ask "What features do users request most frequently?" while a retail brand could focus on "How do customers describe their shopping experience?" These custom queries guide the analysis toward actionable business insights relevant to their market position, customer base, and strategic objectives, ensuring the extracted data directly supports business decision-making processes.'},research:{title:"Research Need",description:'Researchers define custom research questions that align with their specific academic hypotheses and theoretical frameworks. For instance, a sociologist might ask "What are the main themes in discussions about social mobility?" while a psychologist could focus on "How do users express emotional responses to life transitions?" These tailored analytical queries direct the AI to extract insights relevant to their scholarly objectives, enabling systematic examination of social media data while maintaining research rigor and focus on their particular area of study.'}}}],z={frequency:.026,amplitude:.01,maxDistance:.5,isMobile:!1,cameraZ:0,meshType:new a(100,20),geoComplexity:20};function R(){g();const[n,i]=f.useState(z.geoComplexity),[t,o]=f.useState(z.meshType),[s,a]=f.useState(z.frequency),[r,c]=f.useState(z.amplitude),[l,m]=f.useState(z.maxDistance),[p,h]=f.useState(z.isMobile),[v,x]=f.useState(z.cameraZ);return e.jsxs(e.Fragment,{children:[e.jsx(y,{}),e.jsx(d,{data:D,amplitude:r,setAmplitude:c,frequency:s,setFrequency:a,maxDistance:l,setMaxDistance:m,isMobile:p,setIsMobile:h,cameraZ:v,setCameraZ:x,geoComplexity:n,meshType:t},1),e.jsx(u,{}),e.jsx(k,{frequency:.025,amplitude:3.5,maxDistance:10.5,timeSpeed:1,noiseScale:1,noiseDensity:1,noiseOctaves:3,noiseLacunarity:1.8,noiseGain:.4,turbulenceStrength:2,flowDirection:35,waveSpeed:1.2,distortionStrength:1.3,particleSize:10,colorIntensity:3,nearFadeDistance:40,farFadeDistance:160,mouseInfluenceStrength:2,mouseOrderRadius:.52,mouseOrderStrength:.9,chaosStrength:3,meshPositionX:-.5,meshPositionY:.1,meshPositionZ:.4,meshRotationX:.3,meshRotationY:.1,meshRotationZ:.2,cameraPositionX:-1.5,cameraPositionY:.1,cameraPositionZ:1.5,particleColor:[.1,.1,.9]})]})}export{R as default};
